{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('country_vaccinations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "# Remove duplicates\n",
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values (example: fill missing numerical values with the mean)\n",
    "# You can choose to fill missing values for the selected columns here\n",
    "data.fillna(data.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "# Make sure to use the correct column names from your dataset\n",
    "selected_features = ['feature1', 'feature2', 'categorical_feature', 'target_column']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if the selected columns exist in the dataset before selecting them\n",
    "if all(feature in data.columns for feature in selected_features):\n",
    "    data = data[selected_features]\n",
    "else:\n",
    "    print(\"Selected columns do not exist in the dataset. Please verify column names.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical variables (using One-Hot Encoding)\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "encoded_categorical_features = encoder.fit_transform(data[['categorical_feature']])\n",
    "encoded_categorical_feature_names = encoder.get_feature_names(['categorical_feature'])\n",
    "\n",
    "data_encoded = pd.concat([data, pd.DataFrame(encoded_categorical_features, columns=encoded_categorical_feature_names)], axis=1)\n",
    "data_encoded.drop(['categorical_feature'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, and test sets\n",
    "train_data, test_data = train_test_split(data_encoded, test_size=0.2, random_state=42)\n",
    "validation_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling numerical features (example: StandardScaler)\n",
    "scaler = StandardScaler()\n",
    "train_data[['feature1', 'feature2']] = scaler.fit_transform(train_data[['feature1', 'feature2']])\n",
    "validation_data[['feature1', 'feature2']] = scaler.transform(validation_data[['feature1', 'feature2']])\n",
    "test_data[['feature1', 'feature2']] = scaler.transform(test_data[['feature1', 'feature2']])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
